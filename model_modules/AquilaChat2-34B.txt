模型路径: /root/space/models/BAAI/AquilaChat2-34B/main
所有modules:
AquilaForCausalLM(
  (model): AquilaModel(
    (embed_tokens): Embedding(100008, 6144, padding_idx=0)
    (layers): ModuleList(
      (0): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (1): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (2): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (3): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (4): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (5): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (6): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (7): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (8): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (9): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (10): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (11): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (12): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (13): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (14): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (15): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (16): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (17): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (18): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (19): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (20): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (21): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (22): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (23): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (24): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (25): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (26): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (27): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (28): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (29): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (30): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (31): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (32): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (33): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (34): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (35): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (36): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (37): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (38): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (39): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (40): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (41): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (42): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (43): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (44): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (45): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (46): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (47): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (48): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (49): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (50): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (51): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (52): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (53): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (54): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (55): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (56): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (57): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (58): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
      (59): AquilaDecoderLayer(
        (self_attn): AquilaAttention(
          (q_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (k_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (v_proj): Linear(in_features=6144, out_features=1024, bias=False)
          (o_proj): Linear(in_features=6144, out_features=6144, bias=False)
          (rotary_emb): AquilaRotaryEmbedding()
        )
        (mlp): AquilaMLP(
          (gate_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (up_proj): Linear(in_features=6144, out_features=24576, bias=False)
          (down_proj): Linear(in_features=24576, out_features=6144, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): AquilaRMSNorm()
        (post_attention_layernorm): AquilaRMSNorm()
      )
    )
    (norm): AquilaRMSNorm()
  )
  (lm_head): Linear(in_features=6144, out_features=100008, bias=False)
)

可以进行LoRA微调的modules:
[]