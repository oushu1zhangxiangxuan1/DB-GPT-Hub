deepspeed --include v1004:0,1@v1002:0,1 --master_port 29500 dbgpt_hub/train/sft_train.py --deepspeed eyJ0cmFpbl9taWNyb19iYXRjaF9zaXplX3Blcl9ncHUiOiAxLCAiZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzIjogImF1dG8iLCAiZnAxNiI6IHsiZW5hYmxlZCI6ICJhdXRvIn0sICJiZjE2IjogeyJlbmFibGVkIjogImF1dG8ifSwgInRyYWluX2JhdGNoX3NpemUiOiAiYXV0byIsICJhdXRvdHVuaW5nIjogeyJlbmFibGVkIjogdHJ1ZSwgIm1vZGVsX2luZm9fcGF0aCI6ICJhdXRvdHVuaW5nX3Jlc3VsdHMvcHJvZmlsZV9tb2RlbF9pbmZvL21vZGVsX2luZm8uanNvbiIsICJtb2RlbF9pbmZvIjogeyJwcm9maWxlIjogdHJ1ZX0sICJtZXRyaWNfcGF0aCI6ICJhdXRvdHVuaW5nX3Jlc3VsdHMvcHJvZmlsZV9tb2RlbF9pbmZvL21ldHJpY3MuanNvbiJ9LCAiemVyb19vcHRpbWl6YXRpb24iOiB7InN0YWdlIjogM30sICJtZW1vcnlfYnJlYWtfZG93biI6IGZhbHNlfQ== --model_name_or_path /root/space/models/codellama/CodeLlama-13b-Instruct-hf/main --do_train --dataset DBTHub_Prompt-chase-WB-FIO --max_source_length 4096 --max_target_length 256 --template llama2 --finetuning_type lora --lora_rank 2 --lora_alpha 32 --lora_target q_proj --output_dir dbgpt_hub/output/adapter/CodeLlama-13b-Instruct-hf-20240123_194207-xds-auto-qv-DBTHub_Prompt-chase-WB-FIO/ --overwrite_cache --overwrite_output_dir --per_device_train_batch_size 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine_with_restarts --logging_steps 25 --save_steps 1 --learning_rate 2e-4 --num_train_epochs 10000 --plot_loss
